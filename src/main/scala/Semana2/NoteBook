Notas Semana 2.

Conceptos Claves en Spark

Transformaciones vs. Acciones

Transformaciones: Son operaciones lazy (perezosas), lo que significa que no se ejecutan inmediatamente, sino cuando se requiere un resultado.
Acciones: Devuelven un valor al driver y activan la ejecución de las transformaciones previas.

Ejemplo de acción: collect(), que devuelve un array o colección en Scala.
Ejemplo de transformaciones: map, flatMap, filter (devuelven colecciones distribuidas).
explode: Permite aplanar estructuras anidadas, similar a flatMap.

Formatos de Codificación (Encoding)

Apache Avro
Es un formato de serialización muy compacto, rápido y eficiente en el uso de espacio.
Define su esquema utilizando JSON y luego serializa los datos en un formato binario.
Se utiliza para almacenar datos en ficheros, código fuente y servidores (pero no se usa comúnmente para streaming).
Para usarlo en Spark, es necesario agregar la dependencia en build.sbt.


Formatos Columnares

Parquet
Cada columna se almacena en un fichero separado, optimizando la lectura de datos específicos.

ORC (Optimized Row Columnar)
Popular en arquitecturas cloud.
Ideal para conjuntos de datos históricos.
Viene nativo con Spark.

Delta Lake
Formato tipo tableFormat que internamente almacena datos como Parquet.
Se comporta como una tabla de base de datos.
Es necesario configurar Spark para habilitar sus clases específicas.
Se debe gestionar la retención de datos manualmente: https://docs.delta.io/latest/delta-retention.html
No admite merge directamente; se debe agregar manualmente la columna faltante con un valor predeterminado.
No es eficiente para grandes volúmenes de escritura.

Spark SQL en Scala

Configuración de SparkSession
implicit val spark: SparkSession = SparkSession.builder()

Se recomienda declararla como implicit.
builder proporciona una API para construir objetos complejos de manera sencilla.
sparkContext: Devuelve el contexto de Spark, necesario para trabajar con RDDs.
sqlContext: Se utilizaba antes, pero actualmente está deprecated.

DataFrames
Son inmutables.
No es común trabajar con la base de datos default de Spark.
Importante conocer la ruta exacta al importar archivos.



##Técnicas de Optimización en Spark SQL##

Grouping Sets
No se recomienda particionar datos con cardinalidad alta (por ejemplo, ID con valores únicos), ya que generaría muchos ficheros pequeños y afectaría el rendimiento.
Cardinalidad alta significa que hay pocas repeticiones de valores.

Bucketing (Baketizado)
Afecta la manera en que se guardan los datos dentro del fichero.
Se puede ordenar por clave.
Aumenta el tiempo de escritura, pero mejora la lectura al preordenar los datos.

Funciones de Ventana (Window Functions)
Permiten agrupar datos y particionarlos dentro de una ventana de análisis.

Diferencia con groupBy:
Window trabaja a nivel de registro.
groupBy agrupa y devuelve una estructura diferente.

Operaciones con Ventanas
lag: Obtiene el valor de la fila anterior dentro de la ventana.
lead: Obtiene el valor de la fila siguiente dentro de la ventana.

Datasets y Encoders
Son estructuras de datos avanzadas que trabajan con objetos de Scala nativos.
Spark no proporciona Encoder automáticamente.
Si no se configuran los Spark Implicits, no se podrá convertir una secuencia de objetos en un Dataset.

Arquitectura de Spark Cluster

Roles en un Cluster

Master: Coordina la ejecución de los trabajos.
Workers: Ejecutan las tareas asignadas.

Se puede especificar el número de cores y la memoria para los workers.
En el Master, estos parámetros se configuran mediante la clase spark-class en el binario de Spark.

Assembly y Despliegue en Docker
Assembly agrupa todas las dependencias en un único JAR.
En build.sbt, se añade el plugin de assembly para resolver conflictos entre dependencias.

Para desplegar en Docker:
Primero, se ejecuta assembly, por en de no es necesario ejecutarlo separado

Luego, se empaqueta la aplicación en un contenedor Docker.
