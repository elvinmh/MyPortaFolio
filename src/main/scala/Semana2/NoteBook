Importante dentro de cada ejemplo, hay lineas de comentario tanto del maestro como hechas por mi, para garantizar la comprensión,
también, se recomienda ir linea a linea para entender que hace cada función, para ir linea a linea se puede imprimir utilizando:
System.exit(0)

En Spark cuando creamos la SpaksSession para conectar el cluster y estamos trabajando con notebook la primera parte no es
necesario ponerla (Ver referencia del material 02 Apache Spark_2020_V01), porque la variable spark la crea Zeppelin
automáticamente.

¿En qué se diferencia las transformaciones de las acciones? Es que las acciones si le devuelven algo al driver y las
transformaciones son lacy.

La función collect nos devuelve un array, una colección de scala.

Las funciones map, flatmap,filter, devuelven una colección distribuida

Explode permite hacer el flatmap

Encoding:
Formatos
AVRO
Es un enconding, es un formato de serielización que es muy compacto, rápido y eficiente en el uso de espacio,
para definir el esquema utiliza JSON y luego utiliza un formato binario para la serielización.
Serielización es el proceso de estructura de los datos.

Se puede almacenar en ficheros, en código fuentes, en servidores, (los ficheros no suelen utilizarse para streaming de Datos).

Para que ejemplo puieda funcionar, en build.sbt hubo que agregar la dependencia avro.

COLUMNARES / PARQUET
Tienen una particularida, que es como se almacenan, aqui cada columna se almacena en un fichero.

ORC
Suele utilizarse en arquitectura cloud y es mas popular que PARQUET
Aquí viajan los datos, es importante la comprensión.
Cuando hay dataset historicos suele utilizarse
Viene nativo con spark

DELTALAKE
Se llaman tabletFormat.
Internamente lo guarda como PARQUET
Se comportan como tabla de base de datos
Se debe configurar Spark, para que active una serie de clases que son propias de DeltaLake
Para evitar problemas con la retención de datos: https://docs.delta.io/latest/delta-retention.html
Aquí no existe la función merge, se debe agregar manualmente la columna que no se encuentra, con su valor predeterminado.
No es eficiente para gran cantidad volumen de escritura


SPARKSQL sobre Scala
Crear sección
Variable de sección:
implicit val spark: SparkSession = SparkSession.builder() //Siempre creala como una variable implicit
Los objetos builder me dan un APi o una interfaz para construir objetos complejos de una manera sencilla
sparkContext: Devuelve el SparkContext
Esto es necesario para trabajar con RDDs
implicit val sqlContext: SQLContext = spark.sqlContext // Esta variable esta Deprecated, es decir retirada.

Noe s habitual trabajar con la base de datos default

Los DataFrame son inmutable.

MUY IMPORTATNTE:
A la hora de importar fichero, debemos conocer muy bien la dirección e la carpeta.

GroupingSets
No se recomienda utilizar particionamiento para datos con cardinalidades muy alta. Por ej: la columna ID, porque tendrimos fichero muy pequeños.
Cardinalidades muy alta quiere decir las repeticiones son muy pocas.
Esto mataria el rendimiento

El buckering o backetizado  lo que afecta a la manera en que se guardan los datos dentro del fichero, esto tambien se puede ordenar por clave.
Esto tiene un costo en tema de escritura, ya que aumenta el tiempo.
Nos permite guardar los datos pre-ordenado

Funcion Windows:
Sirve para tomar un grupo de Datos y particionarlo.
Diferencia entre operaciones de ventana de GroupBy es que la primera trabajan a nivel de registro, mientras que el segundo agrupa y devuelve una estructura diferente.
Operaciones
lag: Esto permite tener el valor de la siguiente ventana.
lead: Esto es lo contrario, permite obtener el anterior.

Creación de ventanas por secciones

DATASETS
Es un tipo avanzado, es una estructura de datos que trabaja con objetos de Scala nativo.
Encoder, Spark no tiene.


Si no tenemos los SPARK IMPLICITOS, no vamos a poder convertir una secuencia de objeto en un DataSets



